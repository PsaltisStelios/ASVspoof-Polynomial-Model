{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the EER (Simple Example)\n",
    "\n",
    "Take the \"log_eval_score.txt\" and find the EER by using the file_IDs (the logits you MANUALLY find are actually the values in \"log_eval_score.txt\". The title here suggets that log_ stands for logits_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.12124649018477875\n",
      "EER Threshold: 0.2603723704814911\n",
      "False Positive Rate (Min Value): 0.0\n",
      "False Positive Rate (Max Value): 1.0\n",
      "False Negative Rate (Min Value): 0.0\n",
      "False Negative Rate (Max Value): 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure()\\nplt.plot(fpr, fnr, label='DET Curve')\\nplt.xlabel('False Positive Rate')\\nplt.ylabel('False Negative Rate')\\nplt.title('DET Curve')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File paths\n",
    "log_eval_baseline_original_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "# LA_E_1000048 -14.588603\n",
    "\n",
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "# LA_0021 LA_E_1000048 - A08 spoof\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Parse the logits file\n",
    "logits_dict = {}\n",
    "with open(log_eval_baseline_original_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "        logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "        logits_dict[la_id] = logit_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Parse the ground truth file\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]  # Extract the 'LA_E_num' part (Second Column)\n",
    "        label = elements[-1]  # 'bonafide' or 'spoof'\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "EER Calculation: When calculating EER, you don’t set a fixed threshold beforehand. \n",
    "Instead, roc_curve calculates FAR and FRR for a range of thresholds and finds \n",
    "the optimal one where FAR ≈ FRR, which is your EER point.\n",
    "\n",
    "\n",
    "\n",
    "Why Predictions ≠ EER: The line predictions = (probabilities > 0.5).float() is only \n",
    "relevant if you want a specific set of predictions using a fixed threshold. However, EER is \n",
    "calculated without pre-selecting a threshold, as it considers all possible thresholds to find the balance point.\n",
    "'''\n",
    "\n",
    "\n",
    "# Step 3: Match logits with their true labels\n",
    "y_scores = []\n",
    "y_true = []\n",
    "for la_id, score in logits_dict.items():\n",
    "    if la_id in ground_truth_dict:\n",
    "        y_scores.append(score)  # Logit score\n",
    "        y_true.append(ground_truth_dict[la_id])  # Ground truth label\n",
    "\n",
    "# Convert lists to tensors for computation\n",
    "y_scores_tensor = torch.tensor(y_scores)\n",
    "probabilities = torch.sigmoid(y_scores_tensor)  # Convert logits to probabilities\n",
    "#print(probabilities.sort())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Calculate predictions from probabilities\n",
    "predictions = (probabilities > 0.5).float()\n",
    "preds_list = [int(x) for x in predictions.tolist()]\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Calculate EER\n",
    "def calculate_metrics(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold, fpr, fnr\n",
    "\n",
    "\n",
    "# Ensure both lists are NumPy arrays for EER calculation\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(probabilities.tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "eer, eer_threshold, fpr, fnr = calculate_metrics(y_true, y_scores)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"Threshold: {eer_threshold}\")\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"False Negative Rate: {fnr}\")\n",
    "'''\n",
    "\n",
    "\n",
    "# Alternative Step 4: Calculate EER\n",
    "# Using probabilities and true labels directly in EER calculation\n",
    "# NOT USING \"predictions = (probabilities > 0.5).float()\"\n",
    "def calculate_metrics(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold, fpr, fnr\n",
    "\n",
    "# Calculate EER directly\n",
    "eer, eer_threshold, fpr, fnr = calculate_metrics(y_true, probabilities.tolist())\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"EER Threshold: {eer_threshold}\")\n",
    "print(f\"False Positive Rate (Min Value): {min(fpr)}\")\n",
    "print(f\"False Positive Rate (Max Value): {max(fpr)}\")\n",
    "print(f\"False Negative Rate (Min Value): {min(fnr)}\")\n",
    "print(f\"False Negative Rate (Max Value): {max(fnr)}\")\n",
    "\n",
    "\n",
    "# Optional: Plot DET curve for visualization\n",
    "\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(fpr, fnr, label='DET Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('False Negative Rate')\n",
    "plt.title('DET Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function that automates this by caclulating the EER based on 2 file paths (logits and ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.12124649018477875\n",
      "EER Threshold: 0.2603723704814911\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def calculate_eer(logits_file_path, ground_truth_file_path):\n",
    "    # Step 1: Parse the logits file\n",
    "    logits_dict = {}\n",
    "    with open(logits_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "            logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "            logits_dict[la_id] = logit_score\n",
    "\n",
    "    # Step 2: Parse the ground truth file\n",
    "    ground_truth_dict = {}\n",
    "    with open(ground_truth_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[1]  # Extract the 'LA_E_num' part (Second Column)\n",
    "            label = elements[-1]  # 'bonafide' or 'spoof'\n",
    "            ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "    # Step 3: Match logits with their true labels\n",
    "    y_scores = []\n",
    "    y_true = []\n",
    "    for la_id, score in logits_dict.items():\n",
    "        if la_id in ground_truth_dict:\n",
    "            y_scores.append(score)  # Logit score\n",
    "            y_true.append(ground_truth_dict[la_id])  # Ground truth label\n",
    "\n",
    "    # Convert lists to tensors for computation\n",
    "    y_scores_tensor = torch.tensor(y_scores)\n",
    "    probabilities = torch.sigmoid(y_scores_tensor)  # Convert logits to probabilities\n",
    "\n",
    "    # Alternative Step 4: Calculate EER\n",
    "    def calculate_metrics(y_true, y_scores):\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "        fnr = 1 - tpr\n",
    "        eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "        eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "        return eer, eer_threshold, fpr, fnr\n",
    "\n",
    "    # Calculate EER directly\n",
    "    eer, eer_threshold, fpr, fnr = calculate_metrics(np.array(y_true), probabilities.tolist())\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"EER: {eer}\")\n",
    "    print(f\"EER Threshold: {eer_threshold}\")\n",
    "    #print(f\"False Positive Rate (Min Value): {min(fpr)}\")\n",
    "    #print(f\"False Positive Rate (Max Value): {max(fpr)}\")\n",
    "    #print(f\"False Negative Rate (Min Value): {min(fnr)}\")\n",
    "    #print(f\"False Negative Rate (Max Value): {max(fnr)}\")\n",
    "    \n",
    "    #return eer, eer_threshold, fpr, fnr\n",
    "\n",
    "# Example usage:\n",
    "logits_file = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_file = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "calculate_eer(logits_file, ground_truth_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.12124649018477875\n",
      "EER Threshold: 0.2603723704814911\n"
     ]
    }
   ],
   "source": [
    "logits_file = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_file = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "calculate_eer(logits_file, ground_truth_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary containing the ground truth values along the files IDs\n",
    "Dict_Keys = file_ID\n",
    "Dict_Values = Logits of that file_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "# LA_0021 LA_E_1000048 - A08 spoof\n",
    "\n",
    "# Parse the ground truth file\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]  # Extract the 'LA_E_num' part (Second Column)\n",
    "        label = elements[-1]  # 'bonafide' or 'spoof'\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create another dictionary, this time containing the logits per file ID\n",
    "We want 4 seperate dictionaries:\n",
    "1. Baseliine Model Using the Original Dataset\n",
    "2. NCP Special Model Using the Original Dataset\n",
    "3. Baseliine Model Using the Augmented Dataset\n",
    "4. NCP Special Model Using the Augmented Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "log_eval_baseline_original_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "# LA_E_1000048 -14.588603\n",
    "\n",
    "\n",
    "# Parse the logits file\n",
    "logits_origin_baseline_dict = {}\n",
    "with open(log_eval_baseline_original_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "        logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "        logits_origin_baseline_dict[la_id] = logit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "log_eval_NCPspecial_original_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\D_SinglePoly_32_ncp_Special_Or_logits\\baseline_DF\\log_eval_score.txt'\n",
    "# LA_E_1000048 -14.588603\n",
    "\n",
    "\n",
    "# Parse the logits file\n",
    "logits_origin_special_dict = {}\n",
    "with open(log_eval_NCPspecial_original_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "        logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "        logits_origin_special_dict[la_id] = logit_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "log_eval_baseline_augmented_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\A_ASV_Default_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "# LA_E_1000048 -14.588603\n",
    "\n",
    "\n",
    "# Parse the logits file\n",
    "logits_augmented_baseline_dict = {}\n",
    "with open(log_eval_baseline_augmented_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "        logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "        logits_augmented_baseline_dict [la_id] = logit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "log_eval_NCPspecial_augmented_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\D_ASV_Special_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "# LA_E_1000048 -14.588603\n",
    "\n",
    "\n",
    "# Parse the logits file\n",
    "logits_augmented_special_dict = {}\n",
    "with open(log_eval_NCPspecial_augmented_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[0]  # Extract the 'LA_E_num' part (First Column)\n",
    "        logit_score = float(elements[-1])  # Assuming score is the last element\n",
    "        logits_augmented_special_dict[la_id] = logit_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the EER for Each Model ( Seperately )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data /  Baseline Model\n",
      "EER: 0.12124649018477875\n",
      "EER Threshold: 0.2603723704814911\n",
      "\n",
      "Original Data /  NCP Special Model\n",
      "EER: 0.14302880194219994\n",
      "EER Threshold: 0.19067148864269257\n",
      "\n",
      "\n",
      "\n",
      "Augmented Data /  Baseline Model\n",
      "EER: 0.13233689321578773\n",
      "EER Threshold: 0.9178087115287781\n",
      "\n",
      "Augmented Data /  NCP Special Model\n",
      "EER: 0.19723628873058108\n",
      "EER Threshold: 3.499340527923778e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data /  Baseline Model\")\n",
    "calculate_eer(log_eval_baseline_original_path, ground_truth_file)\n",
    "print(\"\")\n",
    "print(\"Original Data /  NCP Special Model\")\n",
    "calculate_eer(log_eval_NCPspecial_original_path, ground_truth_file)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print(\"Augmented Data /  Baseline Model\")\n",
    "calculate_eer(log_eval_baseline_augmented_path, ground_truth_file)\n",
    "print(\"\")\n",
    "print(\"Augmented Data /  NCP Special Model\")\n",
    "calculate_eer(log_eval_NCPspecial_augmented_path, ground_truth_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the EER by applying Logits Fusion per dataset type\n",
    "Each fusioning must happend on each type of dataset. That means we must NOT fusion the logits of two models where one was trained with the original dataset while the other was trained on the augmented dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Dataset (Fusion by Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming `dict1` and `dict2` are the loaded dictionaries containing logits\n",
    "dict1 = logits_origin_baseline_dict\n",
    "dict2 = logits_origin_special_dict\n",
    "\n",
    "\n",
    "# Ensure both dictionaries have the same keys\n",
    "if dict1.keys() != dict2.keys():\n",
    "    raise ValueError(\"The dictionaries must contain the same IDs.\")\n",
    "\n",
    "# Initialize dictionaries to store fused logits for each method\n",
    "average_fused_logits_dict = {}\n",
    "weighted_fused_logits_dict = {}\n",
    "max_pool_fused_logits_dict = {}\n",
    "product_fused_logits_dict = {}\n",
    "harmonic_fused_logits_dict = {}\n",
    "\n",
    "# Define weights for weighted average fusion (example weights; adjust as needed)\n",
    "weights = np.array([0.8, 0.2])\n",
    "\n",
    "for key in dict1.keys():\n",
    "    logits1 = np.array(dict1[key])\n",
    "    logits2 = np.array(dict2[key])\n",
    "\n",
    "    # Check if both logits arrays for the same ID have the same shape\n",
    "    if logits1.shape != logits2.shape:\n",
    "        raise ValueError(f\"Logits for ID {key} have different shapes.\")\n",
    "\n",
    "    # 1. Average of Logits\n",
    "    average_fused_logits = np.mean(np.array([logits1, logits2]), axis=0)\n",
    "    average_fused_logits_dict[key] = average_fused_logits\n",
    "\n",
    "    # 2. Weighted Average of Logits\n",
    "    weighted_fused_logits = np.average(np.array([logits1, logits2]), axis=0, weights=weights)\n",
    "    weighted_fused_logits_dict[key] = weighted_fused_logits\n",
    "\n",
    "    # 3. Max Pooling\n",
    "    max_pool_fused_logits = np.maximum(logits1, logits2)\n",
    "    max_pool_fused_logits_dict[key] = max_pool_fused_logits\n",
    "\n",
    "    # 4. Product of Logits\n",
    "    product_fused_logits = np.prod(np.array([logits1, logits2]), axis=0)\n",
    "    product_fused_logits_dict[key] = product_fused_logits\n",
    "\n",
    "    # 5. Harmonic Mean\n",
    "    # Add a small constant (epsilon) to avoid division by zero\n",
    "    epsilon=1e-10\n",
    "    harmonic_fused_logits = 2 / (1 / (logits1 + epsilon) + 1 / (logits2 + epsilon))\n",
    "    harmonic_fused_logits_dict[key] = harmonic_fused_logits\n",
    "\n",
    "# Convert fused logits to lists for further processing (if needed)\n",
    "average_fused_logits_list = list(average_fused_logits_dict.values())\n",
    "weighted_fused_logits_list = list(weighted_fused_logits_dict.values())\n",
    "max_pool_fused_logits_list = list(max_pool_fused_logits_dict.values())\n",
    "product_fused_logits_list = list(product_fused_logits_dict.values())\n",
    "harmonic_fused_logits_list = list(harmonic_fused_logits_dict.values())\n",
    "\n",
    "# Optional: Display the fused logits\n",
    "#print(\"Average Fused Logits:\", average_fused_logits_dict)\n",
    "#print(\"Weighted Fused Logits:\", weighted_fused_logits_dict)\n",
    "#print(\"Max Pool Fused Logits:\", max_pool_fused_logits_dict)\n",
    "#print(\"Product Fused Logits:\", product_fused_logits_dict)\n",
    "#print(\"Harmonic Fused Logits:\", harmonic_fused_logits_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1272\n",
      "  Average EER Threshold: 0.3927\n",
      "\n",
      "Weighted Average Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1225\n",
      "  Average EER Threshold: 0.3188\n",
      "\n",
      "Max Pooling Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1296\n",
      "  Average EER Threshold: 0.9672\n",
      "\n",
      "Product Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.8802\n",
      "  Average EER Threshold: 1.0000\n",
      "\n",
      "Harmonic Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1380\n",
      "  Average EER Threshold: 0.1172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming the fused logits dictionaries are already defined:\n",
    "# average_fused_logits_dict\n",
    "# weighted_fused_logits_dict\n",
    "# max_pool_fused_logits_dict\n",
    "# product_fused_logits_dict\n",
    "# harmonic_fused_logits_dict\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "k_folds = 5\n",
    "\n",
    "# Function to calculate EER and threshold\n",
    "def calculate_metrics(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# Function to calculate EER across k folds for a given fusion dictionary\n",
    "def calculate_eer_kfold(fused_logits_dict, ground_truth_dict, k_folds=5):\n",
    "    # Prepare keys for cross-validation\n",
    "    keys = list(fused_logits_dict.keys())\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    eers = []\n",
    "    thresholds = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(keys):\n",
    "        y_scores = []\n",
    "        y_true = []\n",
    "        \n",
    "        # Split data into training and testing based on the fold\n",
    "        test_keys = [keys[i] for i in test_idx]\n",
    "        for la_id in test_keys:\n",
    "            if la_id in ground_truth_dict:\n",
    "                y_scores.append(fused_logits_dict[la_id])  # Fused logit score\n",
    "                y_true.append(ground_truth_dict[la_id])  # Ground truth label\n",
    "\n",
    "        # Convert lists to tensors for sigmoid and EER computation\n",
    "        y_scores_tensor = torch.tensor(y_scores)\n",
    "        probabilities = torch.sigmoid(y_scores_tensor).tolist()  # Convert logits to probabilities\n",
    "\n",
    "        # Calculate EER and threshold for this fold\n",
    "        eer, eer_threshold = calculate_metrics(y_true, probabilities)\n",
    "        eers.append(eer)\n",
    "        thresholds.append(eer_threshold)\n",
    "\n",
    "    # Return average EER and threshold across folds\n",
    "    avg_eer = np.mean(eers)\n",
    "    avg_threshold = np.mean(thresholds)\n",
    "    \n",
    "    return avg_eer, avg_threshold\n",
    "\n",
    "# Dictionary to store the results for each fusion method\n",
    "eer_results_kfold = {}\n",
    "\n",
    "# For each fusion method, calculate the k-fold average EER and threshold\n",
    "eer_results_kfold['Average'] = calculate_eer_kfold(average_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Weighted Average'] = calculate_eer_kfold(weighted_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Max Pooling'] = calculate_eer_kfold(max_pool_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Product'] = calculate_eer_kfold(product_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Harmonic'] = calculate_eer_kfold(harmonic_fused_logits_dict, ground_truth_dict)\n",
    "\n",
    "# Print results for each fusion method with k-fold cross-validation\n",
    "for fusion_method, (avg_eer, avg_threshold) in eer_results_kfold.items():\n",
    "    print(f\"{fusion_method} Fusion with {k_folds}-Fold Cross-Validation:\")\n",
    "    print(f\"  Average EER: {avg_eer:.4f}\")\n",
    "    print(f\"  Average EER Threshold: {avg_threshold:.4f}\")\n",
    "    print()  # New line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Dataset (Fusion by Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming `dict1` and `dict2` are the loaded dictionaries containing logits\n",
    "dict1 = logits_augmented_baseline_dict\n",
    "dict2 = logits_augmented_special_dict\n",
    "\n",
    "\n",
    "# Ensure both dictionaries have the same keys\n",
    "if dict1.keys() != dict2.keys():\n",
    "    raise ValueError(\"The dictionaries must contain the same IDs.\")\n",
    "\n",
    "# Initialize dictionaries to store fused logits for each method\n",
    "average_fused_logits_dict = {}\n",
    "weighted_fused_logits_dict = {}\n",
    "max_pool_fused_logits_dict = {}\n",
    "product_fused_logits_dict = {}\n",
    "harmonic_fused_logits_dict = {}\n",
    "\n",
    "# Define weights for weighted average fusion (example weights; adjust as needed)\n",
    "weights = np.array([0.8, 0.2])\n",
    "\n",
    "for key in dict1.keys():\n",
    "    logits1 = np.array(dict1[key])\n",
    "    logits2 = np.array(dict2[key])\n",
    "\n",
    "    # Check if both logits arrays for the same ID have the same shape\n",
    "    if logits1.shape != logits2.shape:\n",
    "        raise ValueError(f\"Logits for ID {key} have different shapes.\")\n",
    "\n",
    "    # 1. Average of Logits\n",
    "    average_fused_logits = np.mean(np.array([logits1, logits2]), axis=0)\n",
    "    average_fused_logits_dict[key] = average_fused_logits\n",
    "\n",
    "    # 2. Weighted Average of Logits\n",
    "    weighted_fused_logits = np.average(np.array([logits1, logits2]), axis=0, weights=weights)\n",
    "    weighted_fused_logits_dict[key] = weighted_fused_logits\n",
    "\n",
    "    # 3. Max Pooling\n",
    "    max_pool_fused_logits = np.maximum(logits1, logits2)\n",
    "    max_pool_fused_logits_dict[key] = max_pool_fused_logits\n",
    "\n",
    "    # 4. Product of Logits\n",
    "    product_fused_logits = np.prod(np.array([logits1, logits2]), axis=0)\n",
    "    product_fused_logits_dict[key] = product_fused_logits\n",
    "\n",
    "    # 5. Harmonic Mean\n",
    "    # Add a small constant (epsilon) to avoid division by zero\n",
    "    epsilon=1e-10\n",
    "    harmonic_fused_logits = 2 / (1 / (logits1 + epsilon) + 1 / (logits2 + epsilon))\n",
    "    harmonic_fused_logits_dict[key] = harmonic_fused_logits\n",
    "\n",
    "# Convert fused logits to lists for further processing (if needed)\n",
    "average_fused_logits_list = list(average_fused_logits_dict.values())\n",
    "weighted_fused_logits_list = list(weighted_fused_logits_dict.values())\n",
    "max_pool_fused_logits_list = list(max_pool_fused_logits_dict.values())\n",
    "product_fused_logits_list = list(product_fused_logits_dict.values())\n",
    "harmonic_fused_logits_list = list(harmonic_fused_logits_dict.values())\n",
    "\n",
    "# Optional: Display the fused logits\n",
    "#print(\"Average Fused Logits:\", average_fused_logits_dict)\n",
    "#print(\"Weighted Fused Logits:\", weighted_fused_logits_dict)\n",
    "#print(\"Max Pool Fused Logits:\", max_pool_fused_logits_dict)\n",
    "#print(\"Product Fused Logits:\", product_fused_logits_dict)\n",
    "#print(\"Harmonic Fused Logits:\", harmonic_fused_logits_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1624\n",
      "  Average EER Threshold: 0.0187\n",
      "\n",
      "Weighted Average Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1332\n",
      "  Average EER Threshold: 0.8563\n",
      "\n",
      "Max Pooling Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1379\n",
      "  Average EER Threshold: 0.9932\n",
      "\n",
      "Product Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.8738\n",
      "  Average EER Threshold: 1.0000\n",
      "\n",
      "Harmonic Fusion with 5-Fold Cross-Validation:\n",
      "  Average EER: 0.1568\n",
      "  Average EER Threshold: 0.0590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming the fused logits dictionaries are already defined:\n",
    "# average_fused_logits_dict\n",
    "# weighted_fused_logits_dict\n",
    "# max_pool_fused_logits_dict\n",
    "# product_fused_logits_dict\n",
    "# harmonic_fused_logits_dict\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "k_folds = 5\n",
    "\n",
    "# Function to calculate EER and threshold\n",
    "def calculate_metrics(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# Function to calculate EER across k folds for a given fusion dictionary\n",
    "def calculate_eer_kfold(fused_logits_dict, ground_truth_dict, k_folds=5):\n",
    "    # Prepare keys for cross-validation\n",
    "    keys = list(fused_logits_dict.keys())\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    eers = []\n",
    "    thresholds = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(keys):\n",
    "        y_scores = []\n",
    "        y_true = []\n",
    "        \n",
    "        # Split data into training and testing based on the fold\n",
    "        test_keys = [keys[i] for i in test_idx]\n",
    "        for la_id in test_keys:\n",
    "            if la_id in ground_truth_dict:\n",
    "                y_scores.append(fused_logits_dict[la_id])  # Fused logit score\n",
    "                y_true.append(ground_truth_dict[la_id])  # Ground truth label\n",
    "\n",
    "        # Convert lists to tensors for sigmoid and EER computation\n",
    "        y_scores_tensor = torch.tensor(y_scores)\n",
    "        probabilities = torch.sigmoid(y_scores_tensor).tolist()  # Convert logits to probabilities\n",
    "\n",
    "        # Calculate EER and threshold for this fold\n",
    "        eer, eer_threshold = calculate_metrics(y_true, probabilities)\n",
    "        eers.append(eer)\n",
    "        thresholds.append(eer_threshold)\n",
    "\n",
    "    # Return average EER and threshold across folds\n",
    "    avg_eer = np.mean(eers)\n",
    "    avg_threshold = np.mean(thresholds)\n",
    "    \n",
    "    return avg_eer, avg_threshold\n",
    "\n",
    "# Dictionary to store the results for each fusion method\n",
    "eer_results_kfold = {}\n",
    "\n",
    "# For each fusion method, calculate the k-fold average EER and threshold\n",
    "eer_results_kfold['Average'] = calculate_eer_kfold(average_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Weighted Average'] = calculate_eer_kfold(weighted_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Max Pooling'] = calculate_eer_kfold(max_pool_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Product'] = calculate_eer_kfold(product_fused_logits_dict, ground_truth_dict)\n",
    "eer_results_kfold['Harmonic'] = calculate_eer_kfold(harmonic_fused_logits_dict, ground_truth_dict)\n",
    "\n",
    "# Print results for each fusion method with k-fold cross-validation\n",
    "for fusion_method, (avg_eer, avg_threshold) in eer_results_kfold.items():\n",
    "    print(f\"{fusion_method} Fusion with {k_folds}-Fold Cross-Validation:\")\n",
    "    print(f\"  Average EER: {avg_eer:.4f}\")\n",
    "    print(f\"  Average EER Threshold: {avg_threshold:.4f}\")\n",
    "    print()  # New line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create light NN for model fusioning AND Ensemble Models (gradient boosting or random forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset (Light NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average EER over 5 folds: 0.1225\n",
      "Average EER Threshold over 5 folds: 0.0483\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Function to load logits into dictionaries\n",
    "def load_logits(file_path):\n",
    "    logits_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[0]\n",
    "            logit_score = float(elements[-1])\n",
    "            logits_dict[la_id] = logit_score\n",
    "    return logits_dict\n",
    "\n",
    "# Load logits data\n",
    "logits1_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "logits2_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\D_SinglePoly_32_ncp_Special_Or_logits\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "logits1_dict = load_logits(logits1_path)\n",
    "logits2_dict = load_logits(logits2_path)\n",
    "\n",
    "# Load ground truth labels\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]\n",
    "        label = elements[-1]\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "# Prepare paired logits and labels\n",
    "paired_logits = []\n",
    "paired_labels = []\n",
    "for la_id in ground_truth_dict:\n",
    "    if la_id in logits1_dict and la_id in logits2_dict:\n",
    "        paired_logits.append([logits1_dict[la_id], logits2_dict[la_id]])\n",
    "        paired_labels.append(ground_truth_dict[la_id])\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.tensor(paired_logits, dtype=torch.float32)\n",
    "labels = torch.tensor(paired_labels, dtype=torch.float32)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Function to calculate EER\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store EERs and thresholds for each fold\n",
    "eers = []\n",
    "thresholds = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train).squeeze()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    ##############################\n",
    "    # Evaluation on the test set #\n",
    "    ##############################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probabilities = model(X_test).squeeze()\n",
    "    \n",
    "    # Calculate EER and threshold for this fold\n",
    "    eer, eer_threshold = calculate_eer(y_test.numpy(), probabilities.numpy())\n",
    "    eers.append(eer)\n",
    "    thresholds.append(eer_threshold)\n",
    "\n",
    "# Calculate average EER and threshold across all folds\n",
    "avg_eer = np.mean(eers)\n",
    "avg_threshold = np.mean(thresholds)\n",
    "\n",
    "# Print the results\n",
    "print(f'Average EER over {k_folds} folds: {avg_eer:.4f}')\n",
    "print(f'Average EER Threshold over {k_folds} folds: {avg_threshold:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset (Gradient Boosting & Random Forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Random Forest EER over 5 folds: 0.1447\n",
      "Average Random Forest Threshold over 5 folds: 0.0780\n",
      "Average Gradient Boosting EER over 5 folds: 0.1211\n",
      "Average Gradient Boosting Threshold over 5 folds: 0.0407\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# File paths\n",
    "logits1_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\A_ASV_Default_32_Or\\baseline_DF\\log_eval_score.txt'\n",
    "logits2_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_Only\\D_SinglePoly_32_ncp_Special_Or_logits\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "\n",
    "# Function to load logits\n",
    "def load_logits(file_path):\n",
    "    logits_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[0]\n",
    "            logit_score = float(elements[-1])\n",
    "            logits_dict[la_id] = logit_score\n",
    "    return logits_dict\n",
    "\n",
    "# Load logits and ground truth labels\n",
    "logits1_dict = load_logits(logits1_path)\n",
    "logits2_dict = load_logits(logits2_path)\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]\n",
    "        label = elements[-1]\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "# Prepare paired logits and labels\n",
    "paired_logits = []\n",
    "paired_labels = []\n",
    "for la_id in ground_truth_dict:\n",
    "    if la_id in logits1_dict and la_id in logits2_dict:\n",
    "        paired_logits.append([logits1_dict[la_id], logits2_dict[la_id]])\n",
    "        paired_labels.append(ground_truth_dict[la_id])\n",
    "\n",
    "# Convert to numpy arrays for sklearn\n",
    "inputs = np.array(paired_logits)\n",
    "labels = np.array(paired_labels)\n",
    "\n",
    "# Define classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# EER calculation function\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# K-Fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "rf_eers = []\n",
    "rf_thresholds = []\n",
    "gb_eers = []\n",
    "gb_thresholds = []\n",
    "\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    # Split data\n",
    "    X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Train classifiers\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    gb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probabilities for the test set\n",
    "    rf_probs = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "    gb_probs = gb_classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate EER and threshold for each classifier on the test set\n",
    "    rf_eer, rf_threshold = calculate_eer(y_test, rf_probs)\n",
    "    gb_eer, gb_threshold = calculate_eer(y_test, gb_probs)\n",
    "    \n",
    "    # Store EERs and thresholds for this fold\n",
    "    rf_eers.append(rf_eer)\n",
    "    rf_thresholds.append(rf_threshold)\n",
    "    gb_eers.append(gb_eer)\n",
    "    gb_thresholds.append(gb_threshold)\n",
    "\n",
    "# Calculate average EER and threshold across all folds\n",
    "avg_rf_eer = np.mean(rf_eers)\n",
    "avg_rf_threshold = np.mean(rf_thresholds)\n",
    "avg_gb_eer = np.mean(gb_eers)\n",
    "avg_gb_threshold = np.mean(gb_thresholds)\n",
    "\n",
    "# Print results\n",
    "print(f'Average Random Forest EER over {k_folds} folds: {avg_rf_eer:.4f}')\n",
    "print(f'Average Random Forest Threshold over {k_folds} folds: {avg_rf_threshold:.4f}')\n",
    "print(f'Average Gradient Boosting EER over {k_folds} folds: {avg_gb_eer:.4f}')\n",
    "print(f'Average Gradient Boosting Threshold over {k_folds} folds: {avg_gb_threshold:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dataset (Light NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average EER over 5 folds: 0.1328\n",
      "Average EER Threshold over 5 folds: 0.0871\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Function to load logits into dictionaries\n",
    "def load_logits(file_path):\n",
    "    logits_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[0]\n",
    "            logit_score = float(elements[-1])\n",
    "            logits_dict[la_id] = logit_score\n",
    "    return logits_dict\n",
    "\n",
    "\n",
    "# Load logits data into dictionaries (using placeholder paths for demonstration)\n",
    "logits1_path =  r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\A_ASV_Default_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "logits2_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\D_ASV_Special_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "logits1_dict = load_logits(logits1_path)\n",
    "logits2_dict = load_logits(logits2_path)\n",
    "\n",
    "# Load ground truth labels\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]\n",
    "        label = elements[-1]\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "# Prepare paired logits and labels\n",
    "paired_logits = []\n",
    "paired_labels = []\n",
    "for la_id in ground_truth_dict:\n",
    "    if la_id in logits1_dict and la_id in logits2_dict:\n",
    "        paired_logits.append([logits1_dict[la_id], logits2_dict[la_id]])\n",
    "        paired_labels.append(ground_truth_dict[la_id])\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.tensor(paired_logits, dtype=torch.float32)\n",
    "labels = torch.tensor(paired_labels, dtype=torch.float32)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Function to calculate EER\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store EERs and thresholds for each fold\n",
    "eers = []\n",
    "thresholds = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train).squeeze()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "        #    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probabilities = model(X_test).squeeze()\n",
    "    \n",
    "    # Calculate EER and threshold for this fold\n",
    "    eer, eer_threshold = calculate_eer(y_test.numpy(), probabilities.numpy())\n",
    "    eers.append(eer)\n",
    "    thresholds.append(eer_threshold)\n",
    "\n",
    "# Calculate average EER and threshold across all folds\n",
    "avg_eer = np.mean(eers)\n",
    "avg_threshold = np.mean(thresholds)\n",
    "\n",
    "# Print the results\n",
    "print(f'Average EER over {k_folds} folds: {avg_eer:.4f}')\n",
    "print(f'Average EER Threshold over {k_folds} folds: {avg_threshold:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dataset (Gradient Boosting & Random Forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Random Forest EER over 5 folds: 0.1605\n",
      "Average Random Forest Threshold over 5 folds: 0.0840\n",
      "Average Gradient Boosting EER over 5 folds: 0.1317\n",
      "Average Gradient Boosting Threshold over 5 folds: 0.0846\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# File paths\n",
    "logits1_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\A_ASV_Default_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "logits2_path = r'C:\\Users\\User\\Desktop\\Final_ASV_Results\\Original_and_Augmented_Raw_5\\D_ASV_Special_32_Augment_5_logits\\baseline_DF\\log_eval_score.txt'\n",
    "ground_truth_path = r'C:\\Users\\User\\Desktop\\Processed_ASV_Data\\Sorted_Metadata\\eval_meta_sort.txt'\n",
    "\n",
    "# Function to load logits\n",
    "def load_logits(file_path):\n",
    "    logits_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            la_id = elements[0]\n",
    "            logit_score = float(elements[-1])\n",
    "            logits_dict[la_id] = logit_score\n",
    "    return logits_dict\n",
    "\n",
    "# Load logits and ground truth labels\n",
    "logits1_dict = load_logits(logits1_path)\n",
    "logits2_dict = load_logits(logits2_path)\n",
    "ground_truth_dict = {}\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        la_id = elements[1]\n",
    "        label = elements[-1]\n",
    "        ground_truth_dict[la_id] = 1 if label == 'bonafide' else 0\n",
    "\n",
    "# Prepare paired logits and labels\n",
    "paired_logits = []\n",
    "paired_labels = []\n",
    "for la_id in ground_truth_dict:\n",
    "    if la_id in logits1_dict and la_id in logits2_dict:\n",
    "        paired_logits.append([logits1_dict[la_id], logits2_dict[la_id]])\n",
    "        paired_labels.append(ground_truth_dict[la_id])\n",
    "\n",
    "# Convert to numpy arrays for sklearn\n",
    "inputs = np.array(paired_logits)\n",
    "labels = np.array(paired_labels)\n",
    "\n",
    "# Define classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# EER calculation function\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# K-Fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "rf_eers = []\n",
    "rf_thresholds = []\n",
    "gb_eers = []\n",
    "gb_thresholds = []\n",
    "\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    # Split data\n",
    "    X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Train classifiers\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    gb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probabilities for the test set\n",
    "    rf_probs = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "    gb_probs = gb_classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate EER and threshold for each classifier on the test set\n",
    "    rf_eer, rf_threshold = calculate_eer(y_test, rf_probs)\n",
    "    gb_eer, gb_threshold = calculate_eer(y_test, gb_probs)\n",
    "    \n",
    "    # Store EERs and thresholds for this fold\n",
    "    rf_eers.append(rf_eer)\n",
    "    rf_thresholds.append(rf_threshold)\n",
    "    gb_eers.append(gb_eer)\n",
    "    gb_thresholds.append(gb_threshold)\n",
    "\n",
    "# Calculate average EER and threshold across all folds\n",
    "avg_rf_eer = np.mean(rf_eers)\n",
    "avg_rf_threshold = np.mean(rf_thresholds)\n",
    "avg_gb_eer = np.mean(gb_eers)\n",
    "avg_gb_threshold = np.mean(gb_thresholds)\n",
    "\n",
    "# Print results\n",
    "print(f'Average Random Forest EER over {k_folds} folds: {avg_rf_eer:.4f}')\n",
    "print(f'Average Random Forest Threshold over {k_folds} folds: {avg_rf_threshold:.4f}')\n",
    "print(f'Average Gradient Boosting EER over {k_folds} folds: {avg_gb_eer:.4f}')\n",
    "print(f'Average Gradient Boosting Threshold over {k_folds} folds: {avg_gb_threshold:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
